{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Paper_summaries.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNuZBrXe5xPovRkS3/EZNgW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kritikalcoder/30-days-of-code/blob/master/Paper_summaries.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzjpRYEjoqjs",
        "colab_type": "text"
      },
      "source": [
        "# **Paper Summaries**\n",
        "\n",
        "### **1. Noiseless Database Privacy**\n",
        "- https://eprint.iacr.org/2011/487.pdf\n",
        "- exploit the entropy already present in the database\n",
        "- no **external** noise added\n",
        "- queries: boolean + real\n",
        "- strong assumption of independence on the database\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MHbeIPUqGDM",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### **2. Differential Privacy without Sensitivity**\n",
        "- https://papers.nips.cc/paper/6050-differential-privacy-without-sensitivity.pdf\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SeuPkZVqJaZ",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### **3. Pain-Free Random Differential Privacy with Sensitivity Sampling**\n",
        "- https://arxiv.org/pdf/1706.02562.pdf \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UPNTLCiqMsI",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### **4. Duet: An Expressive Higher-Order Language and Linear Type System for Statically Enforcing Differential Privacy**\n",
        "- https://sunblaze-ucb.github.io/privacy/papers/duet.pdf  \n",
        "\n",
        "- Uniform stability\n",
        "- Formal type system for sensitivity and privacy budgeting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBdoAsj8qPg3",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### **5. Stability and Generalization of Learning Algorithms that Converge to Global Optima**\n",
        "- http://proceedings.mlr.press/v80/charles18a/charles18a.pdf  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7vJCA0VqTqy",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### **6. Understanding Deep Learning requires re-thinking Generalization**\n",
        "- https://openreview.net/pdf?id=Sy8gdB9xx \n",
        "- tags: generalization, statistical learning theory\n",
        "\n",
        "**Summary + Key Points**  \n",
        "- Aim: Understanding effective capacity of ML models\n",
        "- In principle, these models are rich enough to memorize the training data  \n",
        "- Optimization is empirically easy even when the resulting model does not generalize\n",
        "- Quest to find the true cause of generalization  \n",
        "- What is it that distinguishes NNs that generalize well from those that don't?\n",
        "- Deep NNs easily fit random labels \n",
        "- Explicit regularization may improve generalization performance, but is neither necessary nor sufficient for controlling generalization error\n",
        "- SGD always converges to a solution with small norm\n",
        "- Uniform stability is orthonormal metric to random labeling\n",
        "- Why do certain architectures generalize better than others?\n",
        "- \n",
        "\n",
        "**Key useful insights** \n",
        "- Uniform Stability is the same as l-1 sensitivity\n",
        "- https://en.wikipedia.org/wiki/Stability_(learning_theory)\n",
        "- Property of the algorithm used for training, independent of the data\n",
        "- Uniform stability of an algorithm measures how sensetive the algorithm is to the replacement of a single example\n",
        "- Strong measure of stability\n",
        "- Bounds the generalization error\n",
        "\n",
        "**Regularizations considered**  \n",
        "- weight decay\n",
        "- early stopping (implicit)\n",
        "- data augmentation\n",
        "- dropout\n",
        "- batch normalization (implicit)\n",
        "\n",
        "Rademacher Complexity  \n",
        "VC Dimension  \n",
        "\n",
        "Properly tuned regularizers help improve the generalization performance, but they are not the fundamental reason behind generalization, as the networks continue to perform well without the regularizers.  \n",
        "\n",
        "Generalization error =    \n",
        "\n",
        "Finite Sample Expressivity  \n",
        "A simple 2-layer NN has perfect sample expressivity when the number of parameters is greater than the number of data points"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RHjKRTFpW6z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}